# -*- coding: utf-8 -*-
"""FastCampus.ipynb

Automatically generated by Colaboratory.

This file needs to be run in 'https://colab.research.google.com/'. 
And 2 csv files "winequality-white.csv", "winequality-red.csv" must be uploaded in colab repository '/content/'
"""

import math 
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

plt.style.use('seaborn')
sns.set(font_scale=1)

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)


# ,로 구별된 것이 아니면, 구분자가 무엇인지 명시해줘야 한다(seperate 옵션에 명시)
red = pd.read_csv("/content/winequality-red.csv", sep=";")
white = pd.read_csv("/content/winequality-white.csv", sep=";")

red.info()
white.info()
#<컬럼 설명> fixed acidity: 결합산, volatile acidity: 휘발성 산,citric acid: 시트르산, redisual sugar: 잔당, chlorides: 염화물, free sulfur dioxide: 유리 이산화황, total sulfur dioxide: 이산화황, 
#density: 비중, pH: 산도, sulphates: 황, alcohol: 알콜 도수, quality: 품질(0 ~ 10 사이의 값)

#데이터끼리 합치기 전에 구분자 컬럼 추가해놓기
red['tag']='r'
white['tag']='w'

wine = pd.concat([white, red])
wine

wine.info()

#결측치 시각화해서 보여주는 패키지 불러오기
import missingno

missingno.matrix(wine, figsize=(14, 7))

plt.figure(figsize=(12, 8))
sns.countplot(data = wine, x='quality')

plt.title("Qualitiy Counts", fontsize=15)
plt.xlabel("Quality", fontsize=14)
plt.ylabel("Counts", fontsize=14)

plt.grid()

tag_quality = wine.groupby(['tag'])['quality'].value_counts().unstack(0)
tag_quality

f, ax = plt.subplots(1, 1, figsize=(12, 8))
wine_quality_plot = sns.countplot(data= wine, x= 'quality', hue='tag', palette='pastel')
wine_quality_plot.set_title('Quality per tag', size=24)
wine_quality_plot.set_xlabel("Quality per tag", fontsize=15)
wine_quality_plot.set_ylabel("Count", fontsize=15)
wine_quality_plot.set_xticklabels(wine_quality_plot.get_xticklabels(), rotation=0)
wine_quality_plot.legend(loc='upper right')

#막대 그래프 위에 값 넣어주기
for p in wine_quality_plot.patches:
                      #p.get.bbox() -해당 막대그래프의 왼쪽 시작점, 아래쪽 위치, 오른쪽 끝점, 높이 정보
  left, bottom, width, height = p.get_bbox().bounds #p.get.bbox().bounds - 해당 막대그래프의 정보: 왼쪽 시작점, 아래쪽 위치, 폭, 높이 정보
  wine_quality_plot.annotate("%.0f"%(height), (left+width/2, height+20), ha='center')

for col in wine.columns:
  if col == 'qualtiy' or col == 'tag':
    continue
  sns.boxplot(data = wine, x= 'quality', y=col)
  plt.show()
#음의 상관관계: fixed acidity, volatile acidity, citric acid, chlorides, free sulfur dioxide, total sulfur dioxide, density
#양의 상관관계:

for col in wine.columns:
  if col == 'quality' or col == 'tag':
    continue
  sns.barplot(data = wine, x = 'quality', y=col)
  plt.show()

f2, ax2 = plt.subplots(6, 2, figsize=(25, 15))
for idx, col in enumerate(wine.columns):
  if col == 'tag':
    continue
  idx_level = idx//2
  idx_col = idx%2
  sns.barplot(data = wine, x = 'tag', y=col, ax=ax2[idx_level, idx_col])
plt.show()

f3, ax3 = plt.subplots(1, 1, figsize=(9, 8))

corrmat = wine.corr()
sns.heatmap(corrmat, annot = True, cmap="RdYlBu_r", ax=ax3)
plt.title("Correlaltion map for wine data", size=15)

#와인 데이터가 충분하지 않으므로 퀄리티 별로 데이터를 늘려주기 위해 와인 퀄리티를 단순화
# # 3 ~ 6 점 까지는 나쁜 와인 =0, 7 이상은 좋은 와인 = 1
reviews=[]
for score in wine['quality']:
  if score >= 3 and score < 7:
    reviews.append(0)
  else:
    reviews.append(1)

wine['2_quality'] = reviews

wine.groupby(['tag', '2_quality'])['2_quality'].agg(['count'])

red = wine[wine.tag =='r']
white = wine[wine.tag =='w']

import os

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score

#2_quality 칼럼이 타켓 변수, 나머지 칼럼이 설명 변수가 된다.
#total
Xt = wine.iloc[:, :-3]
yt = wine['2_quality']

#red
Xr = red.iloc[:, :-3]
yr = red['2_quality']

#white
Xw = white.iloc[:, :-3]
yw = white['2_quality']

Xt_train, Xt_test, yt_train, yt_test = train_test_split(Xt, yt, test_size=0.2, random_state = 42) #total
Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr, test_size=0.2, random_state = 42) #red
Xw_train, Xw_test, yw_train, yw_test = train_test_split(Xw, yw, test_size=0.2, random_state = 42) #white

scaler = StandardScaler()

#total
Xt_train = scaler.fit_transform(Xt_train)
Xt_test = scaler.fit_transform(Xt_test)

#red
Xr_train = scaler.fit_transform(Xr_train)
Xr_test = scaler.fit_transform(Xr_test)

#white
Xw_train = scaler.fit_transform(Xw_train)
Xw_test = scaler.fit_transform(Xw_test)

#Logistic Regression Analysis

#total
total_logi = LogisticRegression()
total_logi.fit(Xt_train, yt_train)
predict_total_logi = total_logi.predict(Xt_test)

#red
red_logi = LogisticRegression()
red_logi.fit(Xr_train, yr_train)
predict_red_logi = red_logi.predict(Xr_test)

#white
white_logi = LogisticRegression()
white_logi.fit(Xw_train, yw_train)
predict_white_logi = white_logi.predict(Xw_test)

print("< Accurcy Score >")
print("Total : {:.2f}, Red : {:.2f}, White : {:.2f}".format(accuracy_score(predict_total_logi, yt_test), accuracy_score(predict_red_logi, yr_test), accuracy_score(predict_white_logi, yw_test)))
print("<Classification Report>\n",classification_report(predict_total_logi, yt_test))

#Support Vector Machine(SVM)

#total
svct = SVC()
svct.fit(Xt_train, yt_train)
predict_svct = svct.predict(Xt_test)

#red
svcr = SVC()
svcr.fit(Xr_train, yr_train)
predict_svcr = svcr.predict(Xr_test)

#white
svcw = SVC()
svcw.fit(Xw_train, yw_train)
predict_svcw = svcw.predict(Xw_test)

print("< Accurcy Score >")
print("Total : {:.2f}, Red : {:.2f}, White : {:.2f}".format(accuracy_score(predict_svct, yt_test), accuracy_score(predict_svcr, yr_test), accuracy_score(predict_svcw, yw_test)))
print("<Classification Report>\n",classification_report(predict_svct, yt_test))
print("confusion Matrix:\n", confusion_matrix(predict_svct, yt_test))

#Random Forest Classifier

#total
rfct = RandomForestClassifier()
rfct.fit(Xt_train, yt_train)
predict_rfct = rfct.predict(Xt_test)

#red
rfcr = RandomForestClassifier()
rfcr.fit(Xr_train, yr_train)
predict_rfcr = rfcr.predict(Xr_test)

#white
rfcw = RandomForestClassifier()
rfcw.fit(Xw_train, yw_train)
predict_rfcw = rfcw.predict(Xw_test)

print("< Accurcy Score >")
print("Total : {:.2f}, Red : {:.2f}, White : {:.2f}".format(accuracy_score(predict_rfct, yt_test), accuracy_score(predict_rfcr, yr_test), accuracy_score(predict_rfcw, yw_test)))
print("<Classification Report>\n",classification_report(predict_rfct, yt_test))
print("confusion Matrix:\n", confusion_matrix(predict_rfct, yt_test))

logi_result = [accuracy_score(predict_total_logi, yt_test), accuracy_score(predict_red_logi, yr_test), accuracy_score(predict_white_logi, yw_test)]
svc_result = [accuracy_score(predict_svct, yt_test), accuracy_score(predict_svcr, yr_test), accuracy_score(predict_svcw, yw_test)]
rfc_result = [accuracy_score(predict_rfct, yt_test), accuracy_score(predict_rfcr, yr_test), accuracy_score(predict_rfcw, yw_test)]

df = pd.DataFrame(np.c_[logi_result, svc_result, rfc_result], columns=['Logistic', 'SVM', 'Random Forest'], index=['total', 'red wine', 'white wine'])
df

graph = df.plot(kind='bar', color=['green', 'tomato', 'blue'], edgecolor='lightgray', figsize=(16, 8))

graph.set_title("Result per Model", size=20)
graph.set_xticklabels(graph.get_xticklabels(), rotation=0, size=15)
graph.legend(title='Model', bbox_to_anchor=(1.01, 1))

#막대 그래프 위에 값 표시
for p in graph.patches:
  left, bottom, width, height = p.get_bbox().bounds #막대 그래프의 왼쪽 시작점, 아래쪽 위치, 폭, 높이 정보 전달
  graph.annotate("%.4f"%(height), (left+width/2, height+0.01), ha='center')

plt.ylabel("Accuracy", fontsize=14)
